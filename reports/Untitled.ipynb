{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Generation with secondary structure (WW)\n",
    "\n",
    "## Measures\n",
    "\n",
    "For each generative process we are going to measures several metrics.\n",
    "\n",
    "We want to compare the distribution residue by residue of amino-acids on natural sequences $N_{ind}$ with the one generated by our generatrive process $G$. We then defined :\n",
    "\n",
    "$$H(G, N_{ind}) = - \\mathbb{E}_{X \\sim G} \\log P_{N_{ind}}(x)$$\n",
    "\n",
    "We want to compare it to a DCA model too :\n",
    "\n",
    "$$H(G, N_{DCA}) = - \\mathbb{E}_{X \\sim G} \\log P_{N_{DCA}}(x)$$\n",
    "\n",
    "And finally we want to assess the diversity of amino-acids at each residue :\n",
    "\n",
    "$$H(G, G_{IND}) = - \\mathbb{E}_{X \\sim G} \\log P_{G_{IND}}(x)$$\n",
    "\n",
    "\n",
    "## Rejection sampling \n",
    "\n",
    "### Theory\n",
    "\n",
    "Let's consider an independant model such that $E(x) = \\sum_i g_i(x_i) = g(x)$. We want to extend it to include secondary structure QA. We consider $r$ a pattern a $R$ the set of secondary structure matching pattern $r$.\n",
    "$$M(x) = \\sum_{s \\in R} \\mathbb P(s|x)$$\n",
    "\n",
    "Then $m(x) = - \\log(M(x))$. We can then establish that $E(x) = g(x) + m(x)$. If we want to weight the two value we set $E$ to be $E(x) = g(x) + \\frac{1}{T} m(x)$. We then have :\n",
    "\n",
    "$$\\mathbb{P}(x) = \\frac{1}{Z} G(x)M(x)^{1/T}$$\n",
    "\n",
    "How do we sample according to this probability ? Sampling with G(x) is easy but $M(x)$ is not invertible. We rely on rejection sampling.\n",
    "\n",
    "To do this we sample following to the process :\n",
    "- We sample $X$ according to $G(x)$ and $U$ according to a unifrom distrbituion in $[0,1]$\n",
    "- if $M(X)^{1/T} < U$ or $\\exp\\left(\\frac{1}{T}m(x)\\right)$ we accept the sample, else we reject the sample and go back to step 1\n",
    "\n",
    "We then have :\n",
    "$$\\mathbb{P}(X = x) \\propto G(x)M(x)^{1/T}$$ which allow us to sample with energy $E(x) = g(x) + \\frac{1}{T} m(x)$\n",
    "\n",
    "We can also do the same with RBM after several steps of gibbs sampling\n",
    "\n",
    "### Models\n",
    "\n",
    "**Independant models**\n",
    "\n",
    "So we generate through an independant model and with rejection some samples and have the following results\n",
    "\n",
    "**DCA models**\n",
    "\n",
    "We try to obtan good samples with RBM model by using rejection sampling and simulating the same way an energy of the form :\n",
    "$$ E(x) = E_{RBM}(x) + \\frac{1}{T} m(x) $$\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "Visually independant models led too a lot of samples with bad $E_{DCA}$ : \n",
    "\n",
    "![](img/reject_sampling_edca.png)\n",
    "\n",
    "But in general good structure :\n",
    "\n",
    "![](img/reject_sampling_ss.png)\n",
    "\n",
    "And we have the following metrics : \n",
    "\n",
    "| Generator | H(G\\, N_ind) | H(G\\, N_DCA) | H(G\\, G_ind) | SS |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| natural | 0.27 | 0.30 | 0.26 | 0.80\n",
    "| rbm_seq | 0.28 | 0.33 | 0.28 | 0.78\n",
    "| rbm_seq_ss | 0.27 | 0.32 | 0.27 | 0.80\n",
    "| ind_rejection_sampling_T_3 | 0.27 | **0.41** | 0.27 | 0.79\n",
    "| ind_rejection_sampling_T_10000 | 0.28 | **0.42** | 0.28 | 0.77\n",
    "| rbm_rejection_sampling_T_3 | 0.28 | 0.33 | 0.27 | 0.80\n",
    "| rbm_seq_ss_rejection_sampling_T_3 | 0.27 | 0.32 | 0.26 | 0.81\n",
    "\n",
    "## Maximum Entropy Generators\n",
    "\n",
    "It's harder and harder to lower $T$ because after that all samples get rejected and the model is limited by the limits of the DCA. \n",
    "\n",
    "We need to find a better way to generate samples according to these two metrics. For this reason, I tried the Maximum Entropy Generators from [Kumar, Rithesh, et al. (2019)](https://arxiv.org/pdf/1901.08508.pdf)\n",
    "\n",
    "I tried a first generator as a single dense layer with non-linear activation and a first class of entropy estimators with a bilinear layer.\n",
    "\n",
    "| Generator | H(G\\, N_ind) | H(G\\, N_DCA) | H(G\\, G_ind) | SS |\n",
    "|-----|-----|-----|-----|-----|\n",
    "| natural | 0.27 | 0.30 | 0.26 | 0.80\n",
    "| rbm_seq | 0.28 | 0.33 | 0.28 | 0.78\n",
    "| rbm_seq_ss | 0.27 | 0.32 | 0.27 | 0.80\n",
    "| ind_rejection_sampling_T_3 | 0.27 | **0.41** | 0.27 | 0.79\n",
    "| ind_rejection_sampling_T_10000 | 0.28 | **0.42** | 0.28 | 0.77\n",
    "| rbm_rejection_sampling_T_3 | 0.28 | 0.33 | 0.27 | 0.80\n",
    "| rbm_seq_ss_rejection_sampling_T_3 | 0.27 | 0.32 | 0.26 | **0.81**\n",
    "| meg_without_matching | 0.50 | 0.31 | 0.17 | 0.64\n",
    "| meg_with_matching | **0.62** | **0.42** | 0.16 | **0.81**\n",
    "\n",
    "We take a look at the $E_{DCA}$ :\n",
    "\n",
    "![](img/sampling_edca.png)\n",
    "\n",
    "We take a look at the secondary structures :\n",
    "\n",
    "![](img/sampling_ss.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
