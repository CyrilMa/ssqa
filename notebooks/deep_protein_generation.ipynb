{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from structure.utils import *\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BN = nn.BatchNorm1d\n",
    "\n",
    "class FC_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, N, q = 21, decoder_hidden=[250], decoder_dropout=[0.],\n",
    "                   n_conditions=0):\n",
    "        super(FC_Decoder, self).__init__()\n",
    "        self.N, self.q = N, q\n",
    "        self.n_conditions = n_conditions\n",
    "        last_h = latent_dim\n",
    "        self.lays = []\n",
    "        for h, d in zip(decoder_hidden, decoder_dropout):\n",
    "            self.lays += [nn.Linear(last_h + n_conditions, h), nn.ReLU(),\n",
    "                              nn.Dropout(d)]\n",
    "            last_h = h + n_conditions\n",
    "        self.lays = nn.ModuleList(self.lays)\n",
    "        self.out_lay = nn.Linear(last_h, N*q)\n",
    "\n",
    "    def forward(self, x, z, conditions = None):\n",
    "        h = z\n",
    "        if conditions is not None:\n",
    "            h = torch.cat([h, conditions], 1)\n",
    "        for lay in self.lays:\n",
    "            h = lay(h)\n",
    "            if conditions is not None:\n",
    "                h = torch.cat([h, conditions], 1)\n",
    "        h = self.out_lay(h).view(-1, self.q, self.N)\n",
    "        return F.softmax(h,1)\n",
    "                 \n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, latent_dim, low_res_dim, min_deconv_dim=21,\n",
    "              n_deconv=3, kernel_size=2, BN=nn.BatchNorm1d, dropout=nn.Identity,\n",
    "                max_filters=336):\n",
    "        super(Upsampler, self).__init__()\n",
    "        low_res_features = min(min_deconv_dim * (2**n_deconv), max_filters)\n",
    "        self.low_res_dim, self.low_res_features = low_res_dim, low_res_features\n",
    "        self.linear = nn.Linear(latent_dim, low_res_dim * low_res_features)\n",
    "        self.lays = []\n",
    "        last_h = low_res_features\n",
    "        for i in range(n_deconv):\n",
    "            h = min(min_deconv_dim * 2**(n_deconv-(i+1)), max_filters)\n",
    "            self.lays += [nn.ConvTranspose1d(last_h, h, kernel_size,\n",
    "                                 stride = 2,\n",
    "                                 bias = False), BN(h), nn.PReLU(), dropout()]\n",
    "            last_h = h\n",
    "        self.lays = nn.ModuleList(self.lays)\n",
    "\n",
    "    def forward(self, h):    \n",
    "        h = self.linear(h).view(-1, self.low_res_features, self.low_res_dim)\n",
    "        for lay in self.lays:\n",
    "            h = lay(h)\n",
    "        return h\n",
    "    \n",
    "class RecurrentSequenceDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, N, q = 21, ncell=512,\n",
    "                               project_x=True,\n",
    "                               upsample=False, min_deconv_dim=42,\n",
    "                               input_dropout=nn.Identity, intermediate_dim=63,\n",
    "                               max_filters=336, n_conditions=0,\n",
    "                               cond_concat_each_timestep=False):\n",
    "        super(RecurrentSequenceDecoder, self).__init__()\n",
    "        self.N, self.q = N, q\n",
    "        self.n_conditions = n_conditions\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        self.dropout1 = input_dropout()\n",
    "        if project_x:\n",
    "            self.project = nn.Conv1d(q, q, kernel_size = 1)\n",
    "        if upsample:\n",
    "            self.upsampler = Upsampler(latent_dim + n_conditions, intermediate_dim,\n",
    "                                       min_deconv_dim, n_deconv = 3, max_filters = max_filters)\n",
    "        if upsample:\n",
    "            self.gru = nn.GRU(min_deconv_dim + q, ncell)\n",
    "        else:\n",
    "            self.gru = nn.GRU(latent_dim + n_conditions + q, ncell)\n",
    "        self.conv = nn.Conv1d(ncell, q, 1)\n",
    "            \n",
    "    def forward(self, x, z, conditions):\n",
    "        x = x[:,:self.q]\n",
    "        x = F.pad(x,(1,0))[:,:,:-1]\n",
    "        x = self.dropout1(x)\n",
    "        if self.project is not None:\n",
    "            x = self.project(x)\n",
    "        if conditions is not None:\n",
    "            z = torch.cat([z, conditions], 1)\n",
    "            \n",
    "        if self.upsample:\n",
    "            z = self.upsampler(z)\n",
    "        else:\n",
    "            z = z.view(*z.size(),1).expand(*z.size(), x.size(-1))\n",
    "        h = torch.cat([x,z],1).permute(2,0,1)\n",
    "        h = self.gru(h)[0].permute(1,2,0)\n",
    "        return F.softmax(self.conv(h),1)\n",
    "    \n",
    "class RecurrentStructureDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, N, q = 21, ncell=512,\n",
    "                               project_x=True,\n",
    "                               upsample=False, min_deconv_dim=42,\n",
    "                               input_dropout=nn.Identity, intermediate_dim=63,\n",
    "                               max_filters=336, n_conditions=0,\n",
    "                               cond_concat_each_timestep=False):\n",
    "        super(RecurrentStructureDecoder, self).__init__()\n",
    "        self.N, self.q = N, q\n",
    "        self.n_conditions = n_conditions\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        self.dropout1 = input_dropout()\n",
    "        if project_x:\n",
    "            self.project = nn.Conv1d(q, q, kernel_size = 1)\n",
    "        if upsample:\n",
    "            self.upsampler = Upsampler(latent_dim + n_conditions, intermediate_dim,\n",
    "                                       min_deconv_dim, n_deconv = 3, max_filters = max_filters)\n",
    "        if upsample:\n",
    "            self.gru = nn.GRU(min_deconv_dim + q, ncell)\n",
    "        else:\n",
    "            self.gru = nn.GRU(latent_dim + n_conditions + q, ncell)\n",
    "        self.conv_sequence = nn.Conv1d(ncell, q, 1)\n",
    "        self.conv_structure = nn.Conv1d(ncell, 4, 1)\n",
    "\n",
    "            \n",
    "    def forward(self, x, z, conditions):\n",
    "        x = F.pad(x,(1,0))[:,:,:-1]\n",
    "        x = self.dropout1(x)\n",
    "        if self.project is not None:\n",
    "            x = self.project(x)\n",
    "        if conditions is not None:\n",
    "            z = torch.cat([z, conditions], 1)\n",
    "            \n",
    "        if self.upsample:\n",
    "            z = self.upsampler(z)\n",
    "        else:\n",
    "            z = z.view(*z.size(),1).expand(*z.size(), x.size(-1))\n",
    "        h = torch.cat([x,z],1).permute(2,0,1)\n",
    "        h = self.gru(h)[0].permute(1,2,0)\n",
    "        return F.softmax(self.conv(h),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def size(x, n_conv):\n",
    "    x -= 2\n",
    "    for _ in range(n_conv-1):\n",
    "        x = x//2\n",
    "    return x\n",
    "\n",
    "class FC_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, N, q = 21, encoder_hidden=[250,250,250],\n",
    "               encoder_dropout=[0.7,0.,0.], n_conditions=0):\n",
    "        super(FC_Encoder, self).__init__()\n",
    "        self.N, self.q = N, q\n",
    "        self.n_conditions = n_conditions\n",
    "        last_h = N*q + n_conditions\n",
    "        self.lays = []\n",
    "        for h, d in zip(encoder_hidden, encoder_dropout):\n",
    "            self.lays += [nn.Linear(last_h, h), nn.ReLU(),\n",
    "                              nn.Dropout(d)]\n",
    "            last_h = h\n",
    "        self.lays = nn.ModuleList(self.lays)\n",
    "        self.mean, self.var = nn.Linear(last_h, latent_dim), nn.Linear(last_h, latent_dim)\n",
    "\n",
    "    def forward(self, x, conditions = None):\n",
    "        h = x.view(x.size(0),-1)\n",
    "        if conditions is not None:\n",
    "            h = torch.cat([h, conditions], 1)\n",
    "        for lay in self.lays:\n",
    "            h = lay(h)\n",
    "        z_mean, z_var = self.mean(h), self.var(h)\n",
    "        return z_mean, z_var\n",
    "\n",
    "class Cond_MLP(nn.Module):\n",
    "    def __init__(self, out_dim, n_layers = 2, h = 6, n_conditions = 3):\n",
    "        super(Cond_MLP, self).__init__()\n",
    "        self.n_conditions = n_conditions\n",
    "        last_h = n_conditions\n",
    "        self.lays = []\n",
    "        for i in range(n_layers):\n",
    "            self.lays += [nn.Linear(last_h, h), nn.PReLU()]\n",
    "            last_h = h\n",
    "        self.lays = nn.ModuleList(self.lays)\n",
    "        self.out = nn.Linear(h, out_dim)\n",
    "        self.act = nn.PReLU()\n",
    "\n",
    "    def forward(self, conditions):\n",
    "        h = conditions\n",
    "        for lay in self.lays:\n",
    "            h = lay(h)\n",
    "        return self.act(self.out(h))\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, N, cond_concat_dim = None, q = 21, num_filters=21, kernel_size=2,\n",
    "                BN = True, dropout = nn.Identity(), max_filters = 10000, n_conv = 5,\n",
    "                n_conditions = 0, n_dense_cond = 6):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.N, self.q = N, q\n",
    "        self.n_conditions = n_conditions\n",
    "        last_h = q\n",
    "        self.lays = []\n",
    "        for i in range(n_conv):\n",
    "            h = min(num_filters * (2**i), max_filters)\n",
    "            conv = nn.Conv1d(last_h, h, kernel_size,\n",
    "                       stride = 1 if i==0 else 2, bias=not BN)\n",
    "            batch_norm = nn.BatchNorm1d(h) if BN else nn.Identity()\n",
    "            act = nn.PReLU()\n",
    "            dropout = dropout\n",
    "            self.lays += [conv, batch_norm, act, dropout]\n",
    "            last_h = h\n",
    "        self.lays = nn.ModuleList(self.lays)\n",
    "        size_h = size(N,n_conv) * last_h\n",
    "        cond_concat_dim = size_h if cond_concat_dim is None else cond_concat_dim\n",
    "        if n_conditions>0:\n",
    "            self.cond_mlp = Cond_MLP(cond_concat_dim, n_conditions=n_conditions, h=n_dense_cond)\n",
    "            \n",
    "        self.mean = nn.Linear(size_h + cond_concat_dim if n_conditions > 0 else size_h, latent_dim)\n",
    "        self.var = nn.Linear(size_h + cond_concat_dim if n_conditions > 0 else size_h, latent_dim)\n",
    "\n",
    "    def forward(self, x, conditions = None):\n",
    "        h = x\n",
    "        for lay in self.lays:\n",
    "            h = lay(h)\n",
    "        h = h.view(h.size(0),-1)\n",
    "        if conditions is not None:\n",
    "            h_cond = self.cond_mlp(conditions)\n",
    "            h = torch.cat([h, h_cond], 1)\n",
    "        z_mean, z_var = self.mean(h), self.var(h)\n",
    "        return z_mean, z_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 N, q = 21, n_conditions = 0):\n",
    "        super(ProtVAE, self).__init__()\n",
    "        self.q, self.N = q, N\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.n_conditions = n_conditions\n",
    "    \n",
    "    def encode(self, x, conditions = None):\n",
    "        return self.encoder(x, conditions)\n",
    "    \n",
    "    def decode(self, x, z, conditions = None):\n",
    "        return self.decoder(x, z, conditions)\n",
    "    \n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def forward(self, x, conditions = None):\n",
    "        mu, logvar = self.encode(x, conditions)\n",
    "        z = self.sample(mu, logvar)\n",
    "        \n",
    "        return self.decode(x, z, conditions), mu, logvar\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        return self.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    def save_weights(self, filename):\n",
    "        return torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def prior_sample(self, n_samples = 1, mean = 0, stddev = 1,\n",
    "                     conditions = None, batch_size = 5000):\n",
    "        if n_samples > batch_size:\n",
    "            x = []\n",
    "            total = 0\n",
    "            while total< n_samples:\n",
    "                this_batch = min(batch_size, n_samples - total)\n",
    "                z_sample = mean + stddev * torch.randn(this_batch, self.latent_dim)\n",
    "                x += self._decode(z_sample, conditions)\n",
    "                total += this_batch\n",
    "        else:\n",
    "            z_sample = mean + stddev * torch.randn(this_batch, self.latent_dim)\n",
    "            x = self._decode(z_sample, conditions)\n",
    "        return x\n",
    "    \n",
    "    def _decode(self, z, sample_func=None, conditions=None):\n",
    "        x = torch.zeros(z.size(0), self.q, self.N)\n",
    "        for i in range(self.N):\n",
    "            # iteration is over positions in sequence, which can't be parallelized\n",
    "            x_ = self.decode(x, z, conditions)[:, i, :]\n",
    "            if sample_func is None:\n",
    "                pred_ind = pos_pred.argmax(1) # convert probability to index\n",
    "            else:\n",
    "                pred_ind = sample_func(pos_pred)\n",
    "            for j, p in enumerate(pred_ind):\n",
    "                x[j,p,i] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_Dataset(object):\n",
    "    def __init__(self, file):\n",
    "        data = pickle.load(open(file, 'rb'))\n",
    "        self.length = []\n",
    "        self.primary = np.zeros((len(data), 64, 20))\n",
    "        for i, v in enumerate(data.values()):\n",
    "            self.primary[i,:v.shape[0]] = v[:,:20]\n",
    "            self.length.append(v.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.primary[i], self.length[i]\n",
    "    \n",
    "class HMM_Dataset2(object):\n",
    "    def __init__(self, file):\n",
    "        data = pickle.load(open(file, 'rb'))\n",
    "        self.length = []\n",
    "        self.primary = np.zeros((len(data), 68, 21))\n",
    "        for i, v in enumerate(data.values()):\n",
    "            self.primary[i,:v.shape[0]] = v[:,:40]\n",
    "            self.length.append(v.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.primary[i], self.length[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"/home/cyril/Documents/These/data/\"\n",
    "DATASET = \"PF00397\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HMM_Dataset(f\"{DATA}/{DATASET}/hmm.pkl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16,\n",
    "                        shuffle=True, drop_last=False)\n",
    "\n",
    "# val_dataset = HMM_Dataset(f\"{DATA}/{DATASET}/val.pkl\")\n",
    "# val_loader = DataLoader(val_dataset, batch_size = 16,\n",
    "#                         shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtVAE(\n",
       "  (encoder): CNN_Encoder(\n",
       "    (lays): ModuleList(\n",
       "      (0): Conv1d(20, 21, kernel_size=(2,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): PReLU(num_parameters=1)\n",
       "      (3): Identity()\n",
       "      (4): Conv1d(21, 42, kernel_size=(2,), stride=(2,), bias=False)\n",
       "      (5): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): PReLU(num_parameters=1)\n",
       "      (7): Identity()\n",
       "      (8): Conv1d(42, 84, kernel_size=(2,), stride=(2,), bias=False)\n",
       "      (9): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): PReLU(num_parameters=1)\n",
       "      (11): Identity()\n",
       "      (12): Conv1d(84, 168, kernel_size=(2,), stride=(2,), bias=False)\n",
       "      (13): BatchNorm1d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): PReLU(num_parameters=1)\n",
       "      (15): Identity()\n",
       "      (16): Conv1d(168, 336, kernel_size=(2,), stride=(2,), bias=False)\n",
       "      (17): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (18): PReLU(num_parameters=1)\n",
       "      (19): Identity()\n",
       "    )\n",
       "    (mean): Linear(in_features=1008, out_features=16, bias=True)\n",
       "    (var): Linear(in_features=1008, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): RecurrentSequenceDecoder(\n",
       "    (dropout1): Identity()\n",
       "    (project): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
       "    (gru): GRU(36, 512)\n",
       "    (conv): Conv1d(512, 20, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, N, q = 16, 64, 20\n",
    "device = \"cuda\"\n",
    "enc = CNN_Encoder(latent_dim = h, N = N, q = q)\n",
    "dec = RecurrentSequenceDecoder(latent_dim = h, N = N, q = 20, upsample = False)\n",
    "model = ProtVAE(enc, dec, N).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "threser = (torch.arange(64).view(1,  -1)).to(device)\n",
    "\n",
    "def aa_acc(x, recon_x, length):\n",
    "    thres = (threser<(length.view(-1,1))).int().view(-1)\n",
    "    x = torch.argmax(x, 1).view(-1)\n",
    "    recon_x = torch.argmax(recon_x, 1).view(-1)\n",
    "    return (((x==recon_x).int() * thres)).sum().item()/(thres.sum().item())\n",
    "\n",
    "def loss_function(x_r, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(x_r, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "def train(model, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch_idx, (x,length) in enumerate(train_loader):\n",
    "        x = x.float().permute(0, 2, 1).to(device)\n",
    "        length = length.int().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_r, mu, logvar = model(x)\n",
    "        loss = loss_function(x_r, x[:,:20], mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += torch.clamp(loss, 0, 100000).item()\n",
    "        optimizer.step()\n",
    "        acc = aa_acc(x[:,:20], x_r, length)\n",
    "        train_acc += acc\n",
    "        print(f'''Train Epoch: {epoch} [{100. * batch_idx / len(train_loader):.0f}%]\\tLoss: {train_loss/(batch_idx+1):.2f} || Time: {time.time()-start:.2f}s || Acc: {train_acc/(batch_idx+1):.2f}''', end=\"\\r\")\n",
    "        \n",
    "    print(f'''Train Epoch: {epoch} [100%]\\tLoss: {train_loss/(batch_idx+1):.2f} || Time: {time.time()-start:.2f}s || Acc: {train_acc/(batch_idx+1):.2f}''')\n",
    "    \n",
    "def validation(model, epoch):\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    for batch_idx, x in enumerate(val_loader):\n",
    "        x = x.float().permute(0, 2, 1).to(device)\n",
    "        x_r, mu, logvar = model(x)\n",
    "        loss = loss_function(x_r, x, mu, logvar)\n",
    "        val_loss += loss.item()\n",
    "        acc = aa_acc(x, x_r)\n",
    "        val_acc += acc \n",
    "    print(f'''Val Epoch: {epoch} \\tLoss: {val_loss/(batch_idx+1):.2f} || Time: {time.time()-start:.2f}s || Acc: {val_acc/(batch_idx+1):.2f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [100%]\tLoss: 988.06 || Time: 17.18s || Acc: 0.84\n",
      "Train Epoch: 1 [100%]\tLoss: 988.37 || Time: 19.35s || Acc: 0.84\n",
      "Train Epoch: 2 [100%]\tLoss: 989.17 || Time: 18.69s || Acc: 0.84\n",
      "Train Epoch: 3 [100%]\tLoss: 993.09 || Time: 19.39s || Acc: 0.84\n",
      "Train Epoch: 4 [8%]\tLoss: 975.27 || Time: 1.63s || Acc: 0.84\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-18cea691f3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     validation(model, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-6ced07001238>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train(model, optimizer, epoch)\n",
    "#     validation(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{DATA}/{DATASET}/weights/alignements.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
