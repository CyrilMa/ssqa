{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from config import *\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "sys.path.append(f\"{ROOT}\")\n",
    "PATH = \"/home/malbranke/mutation_data\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_extraction import from_fasta_to_df, from_df_to_fasta, build_profiles\n",
    "from ss_inference.data import SecondaryStructureRawDataset, collate_sequences\n",
    "from ss_inference.model import NetSurfP2, ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_excel(f\"{PATH}/meta.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "name = \"BLAT_ECOLX_Ostermeier2014\"\n",
    "metadata = meta_df.loc[name]\n",
    "\n",
    "family = metadata.family\n",
    "name_dataset = metadata.dataset\n",
    "uniprotid = metadata.uniprot\n",
    "exp_columns = re.findall(r\"\\w\\w*\", metadata.exp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8402 sequences ...\r"
     ]
    }
   ],
   "source": [
    "from_fasta_to_df(f\"{PATH}/{family}\", f\"{PATH}/{family}/{family}.fasta\", chunksize=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniformize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    L = np.zeros((m + 1, n + 1))\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i - 1] == Y[j - 1] or Y[j-1] == \"-\":\n",
    "                L[i, j] = L[i - 1, j - 1] + 1\n",
    "            else:\n",
    "                L[i, j] = 0\n",
    "\n",
    "    i, j = np.unravel_index(L.argmax(), L.shape)\n",
    "    posX, posY = [], []\n",
    "    while i > 0 and j > 0:\n",
    "        if L[i, j] == 0:\n",
    "            break\n",
    "        if X[i - 1] == Y[j - 1]:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            posX.append(i)\n",
    "            posY.append(j)\n",
    "            continue\n",
    "    posX.sort(), posY.sort()\n",
    "    return len(posX), (min(posX), max(posX), min(posY), max(posY)), L\n",
    "\n",
    "\n",
    "def replace(seq, idx, end):\n",
    "    i = int(idx)\n",
    "    return seq[:i]+end+seq[i+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "nat_df = pd.read_csv(f\"{PATH}/{family}/sequences.csv\")\n",
    "mut_df = pd.read_csv(f\"{PATH}/{family}/{family}_{name_dataset}.csv\", comment = \"#\", sep=\";\")\n",
    "\n",
    "mut_df[\"origin\"] = mut_df.mutant.apply(lambda x : x[0] if len(x) > 2 else None)\n",
    "mut_df[\"end\"] = mut_df.mutant.apply(lambda x : x[-1] if len(x) > 2 else None)\n",
    "mut_df[\"idx\"] = mut_df.mutant.apply(lambda x : int(x[1:-1])-1 if len(x) > 2 else -1)\n",
    "\n",
    "seq_nat  = nat_df.loc[0].seq\n",
    "seq_mut = \"\" \n",
    "for i in range(int(max(mut_df[\"idx\"]))+1):\n",
    "    try:\n",
    "        seq_mut += mut_df[mut_df.idx == i].reset_index().origin[0]\n",
    "    except:\n",
    "        seq_mut += \"-\"\n",
    "\n",
    "_, (m_nat, M_nat, m_mut, M_mut), _ = lcs(seq_nat, seq_mut)\n",
    "\n",
    "mut_df = mut_df[(mut_df.idx <= M_mut) | (mut_df.mutant == \"WT\")]\n",
    "mut_df = mut_df[(mut_df.idx >= m_mut) | (mut_df.mutant == \"WT\")]\n",
    "mut_df.idx = mut_df.idx - m_mut + m_nat\n",
    "\n",
    "seq_mut2 = \"\" \n",
    "for i in range(int(max(mut_df[\"idx\"]))+1):\n",
    "    try:\n",
    "        seq_mut2 += mut_df[mut_df.idx == i].reset_index().origin[0]\n",
    "    except:\n",
    "        seq_mut2 += \"-\"\n",
    "print(seq_mut2 == seq_nat[m_nat:M_nat+1] )\n",
    "    \n",
    "mut_df[\"name\"] = mut_df.apply(lambda x : (x.origin + str(x.idx) + x.end) if x.mutant != \"WT\" else \"WT\", axis = 1)\n",
    "mut_df[\"seq\"] = seq_nat\n",
    "mut_df[\"seq\"] = mut_df.apply(lambda x : replace(x.seq, x.idx, x.end) if x.mutant != \"WT\" else x.seq, axis = 1)\n",
    "mut_df[\"aligned_seq\"] = mut_df[\"seq\"]\n",
    "mut_df = mut_df.set_index(\"name\")\n",
    "\n",
    "mut_df.to_csv(f\"{PATH}/{family}/{name_dataset}_mutation_sequences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4783 sequences ...\r"
     ]
    }
   ],
   "source": [
    "from_df_to_fasta(f\"{PATH}/{family}\", f\"{PATH}/{family}/{name_dataset}_mutation_sequences.csv\", prefix = f\"{dataset}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8353 sequences ...\r"
     ]
    }
   ],
   "source": [
    "from_df_to_fasta(f\"{PATH}/{family}\", f\"{PATH}/{family}/sequences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make HMM profile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='hhmake -i /home/malbranke/mutation_data/BLAT_ECOLX/aligned.fasta -M 100', returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"make HMM profile\")\n",
    "subprocess.run(f'hhmake -i {PATH}/{family}/aligned.fasta -M 100', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [00:24<00:00, 192.84it/s]\n"
     ]
    }
   ],
   "source": [
    "build_profiles(f\"{PATH}/{family}\", prefix = f\"{name_dataset}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary structure\n",
      "Model -50\n"
     ]
    }
   ],
   "source": [
    "dataset = SecondaryStructureRawDataset(f\"{PATH}/{family}/{name_dataset}_hmm.pkl\")\n",
    "\n",
    "print(\"Secondary structure\")\n",
    "loader = DataLoader(dataset, batch_size = 1,\n",
    "                        shuffle=False, drop_last=False, collate_fn = collate_sequences)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model_ss3 = NetSurfP2(50, \"\")\n",
    "model_ss3 = model_ss3.to(device)\n",
    "model_ss3.load_state_dict(torch.load(f\"{DATA}/secondary_structure/lstm_50feats.h5\"))\n",
    "print(model_ss3)\n",
    "\n",
    "#_, _, ss3 = model_ss3.predict(loader)\n",
    "#pickle.dump(ss3, open(f\"{PATH}/{family}/{dataset}_ss3.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern_matching.utils import *\n",
    "\n",
    "import biotite\n",
    "import biotite.structure as struc\n",
    "\n",
    "import biotite.database.rcsb as rcsb\n",
    "import biotite.structure.io.mmtf as mmtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1AXB', '1BT5', '1BTL', '1CK3', '1ERM', '1ERO', '1ERQ', '1ESU',\n",
       "       '1FQG', '1JTD', '1JTG', '1JVJ', '1JWP', '1JWV', '1JWZ', '1LHY',\n",
       "       '1LI0', '1LI9', '1M40', '1NXY', '1NY0', '1NYM', '1NYY', '1PZO',\n",
       "       '1PZP', '1S0W', '1TEM', '1XPB', '1XXM', '1YT4', '1ZG4', '1ZG6',\n",
       "       '2B5R', '2V1Z', '2V20', '3C7U', '3C7V', '3CMZ', '3DTM', '3JYI',\n",
       "       '3TOI', '4DXB', '4DXC', '4GKU', '4IBR', '4IBX', '4ID4', '4MEZ',\n",
       "       '4QY5', '4QY6', '4R4R', '4R4S', '4RVA', '4RX2', '4RX3', '4ZJ1',\n",
       "       '4ZJ2', '4ZJ3', '5HVI', '5HW1', '5HW5', '5I52', '5I63', '5IQ8',\n",
       "       '5KKF', '5KPU', '5NPO', '6APA', '6AYK', '6B2N'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_uniprot = pd.read_csv(f\"{DATA}/cross/uniprot_pdb.csv\", index_col=0)\n",
    "pdb_uniprot[pdb_uniprot.uni == uniprotid].pdb.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest, patterns = 0, []\n",
    "c = 'A'\n",
    "for pdb in pdb_uniprot[pdb_uniprot.uni == uniprotid].pdb.values:\n",
    "    file_name = rcsb.fetch(pdb, \"mmtf\", biotite.temp_dir())\n",
    "    mmtf_file = mmtf.MMTFFile()\n",
    "    mmtf_file.read(file_name)\n",
    "    array = mmtf.get_structure(mmtf_file, model=1)\n",
    "    # Transketolase homodimer\n",
    "    tk_dimer = array[struc.filter_amino_acids(array)]\n",
    "    # Transketolase monomer\n",
    "    tk_mono = tk_dimer[tk_dimer.chain_id == c]\n",
    "\n",
    "    chain_id_per_res = array.chain_id[struc.get_residue_starts(tk_dimer)]\n",
    "    chain_idx = np.where(chain_id_per_res == c)[0]\n",
    "    ss_seq = np.array(list(mmtf_file[\"entityList\"][0][\"sequence\"]))[chain_idx]\n",
    "    length, (m_nat, M_nat, m_mut, M_mut), _= lcs(seq_nat, \"\".join(ss_seq))\n",
    "    if length < longest:\n",
    "        continue\n",
    "    if length > longest:\n",
    "        longest = length\n",
    "        patterns = []\n",
    "    sse = mmtf_file[\"secStructList\"]\n",
    "    sse = sse[:chain_id_per_res.shape[0]][chain_id_per_res == c]\n",
    "    sse = np.array(sse[m_mut: M_mut + 1])\n",
    "    sse = np.array([sec_struct_codes[code] for code in sse], dtype=\"U1\")\n",
    "    sse = np.array([dssp_to_abc[e] for e in sse], dtype=\"U1\")\n",
    "    sse = to_onehot([abc_codes[x] for x in sse], (None, 3))\n",
    "    dss = (sse[1:] - sse[:-1])\n",
    "    cls = to_onehot(np.where(dss == -1)[1], (None, 3)).T\n",
    "    bbox = np.array([np.where(dss == 1)[0], np.where(dss == -1)[0], *cls]).T\n",
    "    pat = np.argmax(bbox[:, 2:], 1)\n",
    "\n",
    "    patterns.append(pat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_patterns, n_patterns = [], []\n",
    "for pat in patterns:\n",
    "    char_pat = \"\".join([\"abc\"[x] for x in pat])\n",
    "    if len(char_pat):\n",
    "        c_patterns.append(char_pat)\n",
    "        n_patterns.append(list(pat))\n",
    "max_occ, c_pattern, n_pattern = 0, None, None\n",
    "for c, n in zip(c_patterns, n_patterns):\n",
    "    n_occ = c_patterns.count(c)\n",
    "    if n_occ > max_occ:\n",
    "        max_occ = n_occ\n",
    "        c_pattern, n_pattern = c, n\n",
    "c_patterns = [c_pattern]\n",
    "n_patterns = [n_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern_matching.utils import *\n",
    "from pattern_matching.pattern import *\n",
    "from pattern_matching.loss import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 263\n",
    "Q = np.ones((3, size+1, size+1)) * (-np.inf)\n",
    "e = size\n",
    "for i in range(size+1):\n",
    "    Q[:3, i, i+1:] = 0\n",
    "Q = Q.reshape(1, *Q.shape)\n",
    "\n",
    "regex = ([(i,None,None) for i in n_pattern])\n",
    "\n",
    "seq_hmm = torch.load(f\"{PATH}/{family}/hmm.pt\")\n",
    "\n",
    "matcher = PatternMatching(model_ss3, pattern = regex, Q = Q,\n",
    "                          seq_hmm = seq_hmm, ss_hmm = None, \n",
    "                          size = size, name = c_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary structure\n"
     ]
    }
   ],
   "source": [
    "dataset = SecondaryStructureRawDataset(f\"{PATH}/{family}/{name_dataset}_hmm.pkl\")\n",
    "\n",
    "print(\"Secondary structure\")\n",
    "loader = DataLoader(dataset, batch_size = 200,\n",
    "                        shuffle=False, drop_last=False, collate_fn = collate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [32:49, 246.13s/it]"
     ]
    }
   ],
   "source": [
    "ls, M, L = [],[],[]\n",
    "len_pat = len(matcher.pattern)\n",
    "for batch_idx, data in tqdm(enumerate(loader)):\n",
    "    x = data[0].permute(0,2,1).float()\n",
    "    torch.cuda.empty_cache()\n",
    "    m = Matching(x)\n",
    "    matcher(m)\n",
    "    L.append(m.L)\n",
    "    ls.append(m.ls)\n",
    "    M.append(m.M)\n",
    "    del m\n",
    "ls= torch.cat(ls,0)\n",
    "M = torch.cat(M,0)\n",
    "L = torch.cat(L,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
