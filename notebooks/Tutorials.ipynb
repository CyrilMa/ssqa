{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSQA - Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, random, warnings, subprocess, torch\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we recall the full pipeline for Pattern Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PFAM_DATA\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_data(f\"{DATA}/{DATASET}\", \"full.fasta\")\n",
    "structfam = get_structures(DATA, DATASET)\n",
    "build_patterns(structfam, f\"{DATA}/{DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset `DATASET` and the file `filename` where all **aligned** sequences are. The data file `data.pt` will be stored in `PFAM_DATA/DATASET` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"russ\"\n",
    "filename = \"aligned.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the two first steps :\n",
    "- Clustering and splitting the clusters between training and testing set with MMSEQS\n",
    "- Build HMMer profile with HHsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_protein_df(f\"{PFAM_DATA}/{DATASET}\", filename)\n",
    "\n",
    "# We build clusters with MMSEQS\n",
    "subprocess.run(\n",
    "    f'mmseqs easy-cluster \"{PFAM_DATA}/{DATASET}/unaligned.fasta\" \"{DATA}/{DATASET}/tmp/clusters.tsv\" \"{DATA}/{DATASET}/tmp\" --min-seq-id 0.7',\n",
    "    shell=True)\n",
    "\n",
    "# We compute cluster weights\n",
    "cluster_weights(folder)\n",
    "\n",
    "# We split between training and validation set (useful for training RBM)\n",
    "split_train_val_set(folder)\n",
    "\n",
    "# We compute profiles\n",
    "subprocess.run(f'hhmake -i {PFAM_DATA}/{DATASET}/aligned.fasta -M 100', shell=True)\n",
    "build_profiles(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step consists in retrieving the pattern, for this, three methods are available :\n",
    "1. Retrieving the specific structure from a known `uniprot_id`\n",
    "2. Retrieving available structure from the PFAM family `pfam_id`\n",
    "3. Use the `PatternInference` to retrieve a pattern by inference (if no pattern available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 : Retrieving the specific structure from a known `uniprot_id`\n",
    "\n",
    "uniprot_id = \"P0A9J8\"\n",
    "nat_seq = \"TSENPLLALREKISALDEKLLALLAERRELAVEVGKAKLLSHRPVRDIDRERDLLERLITLGKAHHLDAHYITRLFQLIIEDSVLTQQALLQQH\"\n",
    "search_pattern(f\"{DATA}/{MUT_DATASET}\", uniprot_id, nat_seq)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 : Retrieving available structure from the PFAM family `pfam_id`\n",
    "\n",
    "strucfam = get_structures(DATASET)\n",
    "build_patterns(structfam, f\"{PFAM_DATA}/{DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 : Use the `PatternInference` to retrieve a pattern by inference (if no pattern available)\n",
    "\n",
    "infer_pattern(f\"{PFAM_DATA}/{DATASET}\", indices = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Secondary Structure Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The first thing to do is to train the model if it is not done (a trained model is available in `data/utils`. Here is the way to do it if you wish so with the adapted `NetSurfP2`.\n",
    "\n",
    "Reference : \n",
    "\n",
    "*NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning, Klausen, Michael Schantz and Jespersen, Martin Closter and Nielsen, Henrik and Jensen, Kamilla Kjaergaard and Jurtz, Vanessa Isabell and Soenderby, Casper Kaae and Sommer, Morten Otto Alexander and Winther, Ole and Nielsen, Morten and Petersen, Bent and others*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from data import SecondaryStructureAnnotatedDataset, collate_sequences_train\n",
    "from ss_inference import NetSurfP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first retrieve the dataset we collected and reformate from `NetSurfP2`. The training and validation set are available at `data/utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SecondaryStructureAnnotatedDataset(f\"{UTILS}/training_set.pt\", 50)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 15, collate_fn = collate_sequences_train,\n",
    "                        shuffle = True, drop_last=True)\n",
    "\n",
    "val_dataset = SecondaryStructureAnnotatedDataset(f\"{UTILS}/validation_set.pt\", 50)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 15, collate_fn = collate_sequences_train,\n",
    "                        shuffle=False, drop_last=False)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = NetSurfP2(50, name=\"netsurp2\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train, 5 epochs should be enough with these parameters to reach a palier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = 0\n",
    "for i in range(5):\n",
    "    model.train_epoch(train_loader, optimizer, i)\n",
    "    mean_ss3_acc, _ = model.val_epoch(val_loader, i)\n",
    "    if mean_ss3_acc > max_acc:\n",
    "        torch.save(model.state_dict(), f\"{UTILS}/nsp2_50feats.h5\")\n",
    "        max_acc = mean_ss3_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "\n",
    "Once training is done, model is then ready for use. We load a dataset using `SSQAData_SSinf` and `collate_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataset = SSQAData_SSinf(f\"{PFAM_DATA}/{DATASET}/data.pt\")\n",
    "loader = DataLoader(dataset, batch_size = batch_size, \n",
    "                          shuffle = False, drop_last=False, collate_fn = collate_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ss = NetSurfP2(50, \"nsp\")\n",
    "model_ss = model_ss.to(device)\n",
    "optimizer = optim.Adam(model_ss.parameters(), lr=0.001)\n",
    "model_ss.load_state_dict(torch.load(f\"{UTILS}/nsp2_50feats.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss3 = torch.zeros(len(dataset), 3, 100)\n",
    "ss8 = torch.zeros(len(dataset), 8, 100)\n",
    "\n",
    "for batch_idx, data in enumerate(loader):\n",
    "    x = torch.tensor(data[0]).float().cuda()\n",
    "    _, s8, s3 = model_ss(x)\n",
    "    ss_ = F.softmax(s3,1).detach().cpu()\n",
    "    ss3[batch_size*batch_idx: batch_size*(batch_idx+1), :, :ss_.size(-1)] = ss_\n",
    "    ss_ = F.softmax(s8,1).detach().cpu()\n",
    "    ss8[batch_size*batch_idx: batch_size*(batch_idx+1), :, :ss_.size(-1)] = ss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - SSQA\n",
    "\n",
    "For an overview of how to handle, please refer to `Chorismate Mutase - Russ et al. 2020` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Restricted Boltzman Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SSQAData_RBM(f\"{DATA}/{DATASET}/data.pt\")\n",
    "loader = DataLoader(dataset, batch_size = 100, shuffle = True)\n",
    "batch_size, q, N = dataset.seqs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pots = torch.zeros(q+1, N)\n",
    "for w, v in zip(dataset.weights,dataset):\n",
    "    pots += w*v\n",
    "pots /= torch.sum(dataset.weights)\n",
    "pots = (pots-pots.mean(0)[None]).view(-1).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_layers = [\"sequence\"]\n",
    "hidden_layers = [\"hidden\"]\n",
    "\n",
    "v = OneHotLayer(pots, N=N, q=q+1, name=\"sequence\")\n",
    "h = GaussianLayer(N=200, name=\"hidden\")\n",
    "\n",
    "E = [(v.name, h.name)]\n",
    "\n",
    "model_rbm = MRF(layers = {v.name: v,\n",
    "                    h.name: h}, edges = E, name = \"\")\n",
    "\n",
    "for visible in visible_layers:\n",
    "    edge = model_rbm.get_edge(visible, \"hidden\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#model.load(f\"{DATA}/{DATASET}/weights/seq-reg-200_4320.h5\")\n",
    "#model.ais(n_inter = 2000, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40000):\n",
    "    model.train_epoch(optimizer, loader, visible_layers, hidden_layers, [gamma], epoch,\n",
    "          savepath=f\"{PFAM_DATA}/{DATASET}/weights/seq-reg-200\")\n",
    "    if not epoch % 30:\n",
    "        model.val(val_loader, visible_layers, hidden_layers, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Sampling with RBM and SSQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_layers = [\"sequence\"]\n",
    "hidden_layers = [\"hidden\"]\n",
    "file_weights\n",
    "\n",
    "v = OneHotLayer(pots, N=N, q=q+1, name=\"sequence\")\n",
    "h = GaussianLayer(N=200, name=\"hidden\")\n",
    "\n",
    "E = [(v.name, h.name)]\n",
    "\n",
    "model_rbm = MRF(layers = {v.name: v,\n",
    "                    h.name: h},\n",
    "            edges = E,\n",
    "            name = \"\")\n",
    "\n",
    "for visible in visible_layers:\n",
    "    edge = model1.get_edge(visible, \"hidden\")\n",
    "    \n",
    "model_rbm.load(f\"{DATA}/{DATASET}/weights/{file_weights}.h5\")\n",
    "model_rbm.ais(n_inter = 2000, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
