{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pgm.layers import MarkovChainLayer, OneHotLayer, Layer\n",
    "from pgm.data import Seq_SS_Data\n",
    "from pgm.ebm import EnergyModel\n",
    "from pgm.nn import ConvNet, ConvBlock\n",
    "from pgm.metrics import hinge_loss, aa_acc\n",
    "from pgm.utils import I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"/home/cyril/Documents/These/data/secondary_structure\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K','L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "keys = [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 27]\n",
    "aa_dict = {k:i for i,k in enumerate(keys)}\n",
    "\n",
    "AAINDEX = \"/home/cyril/Documents/These/data/aaindex\"\n",
    "df_index = pd.read_csv(f\"{AAINDEX}/aa_index.csv\", index_col = 0)\n",
    "df_index[aa] = ((df_index[aa].values.T - df_index[aa].mean(1).values)/df_index[aa].std(1).values).T\n",
    "df_index = df_index.fillna(0)\n",
    "AA_MAT = df_index[aa].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_onehot(a, shape):\n",
    "    onehot = np.zeros(shape)\n",
    "    onehot[np.arange(len(a)), a] = 1\n",
    "    return onehot\n",
    "\n",
    "def to_aaindex(a, size):\n",
    "    aaindex = np.zeros((size, AA_MAT.shape[1]))\n",
    "    for i, x in enumerate(a):\n",
    "        if x == 26:\n",
    "            aaindex[i] = np.zeros(AA_MAT.shape[1])\n",
    "        else:\n",
    "            aaindex[i] = AA_MAT[aa_dict[x]]\n",
    "    return aaindex\n",
    "\n",
    "class Seq_SS_Data(object):\n",
    "    def __init__(self, file, size = 512):\n",
    "        self.primary, self.ss3 = [], []\n",
    "        df = pd.read_json(file)\n",
    "        df[\"length\"] = df.primary.apply(lambda x : len(x))\n",
    "        df = df[df.length <= size]\n",
    "        \n",
    "        self.length = list(df.length)\n",
    "        self.primary = list(df.primary.apply(lambda d : np.array(d)))\n",
    "        self.ss3 = list(df.ss3.apply(lambda d : np_onehot(d, (size, 3))))\n",
    "        del df\n",
    "        self.ss3 = np.array(self.ss3)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.primary)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.length[i] <= 128:\n",
    "            return to_aaindex(self.primary[i], 128), self.ss3[i, :128], self.length[i]\n",
    "        cursor = np.random.randint(0, self.length[i]-128)\n",
    "        return to_aaindex(self.primary[i][cursor:cursor+128], 128), self.ss3[i, cursor:cursor+128], self.length[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Seq_SS_Data(f\"{DATA}/secondary_structure_train.json\", size = 512)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, \n",
    "                        shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Seq_SS_Data(f\"{DATA}/secondary_structure_valid.json\", size = 512)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, \n",
    "                        shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((3, 3))\n",
    "for seq, length in zip(train_dataset.ss3, train_dataset.length):\n",
    "    seq = np.argmax(seq, -1)\n",
    "    x = seq[0]\n",
    "    y = seq[0]\n",
    "    for i in range(1, length):\n",
    "        x, y = y, seq[i]\n",
    "        t[x,y] += 1\n",
    "t /= np.sum(t, 1)\n",
    "t = torch.tensor(t).float()\n",
    "t = (t+t.t())/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(): \n",
    "    return nn.LeakyReLU(0.2, inplace = False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, bias=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        pad = (kernel_size-1)//2\n",
    "        self.conv_1 = ConvBlock(nn.Conv1d, leaky_relu, nn.BatchNorm1d, \n",
    "                                in_channels, out_channels, kernel_size,\n",
    "                                stride=1, padding=pad, bias=bias)\n",
    "        self.conv2 = ConvBlock(nn.Conv1d, None, nn.BatchNorm1d, \n",
    "                                in_channels, out_channels, 1,\n",
    "                                stride=1, padding=0, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv_1(x)\n",
    "        identity = self.conv2(x)\n",
    "        out += identity\n",
    "        return out\n",
    "    \n",
    "class ResBlock2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.conv_11 = ConvBlock(nn.Conv1d, leaky_relu, nn.BatchNorm1d, \n",
    "                                in_channels, out_channels, 3,\n",
    "                                stride=1, padding=1, bias=bias)\n",
    "        self.conv_12 = ConvBlock(nn.Conv1d, leaky_relu, nn.BatchNorm1d, \n",
    "                                out_channels, out_channels, 3,\n",
    "                                stride=2, padding=1, bias=bias)\n",
    "        self.conv2 = ConvBlock(nn.Conv1d, None, None, \n",
    "                                in_channels, out_channels, 3,\n",
    "                                stride=2, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv_11(x)\n",
    "        out = self.conv_12(out)\n",
    "        identity = self.conv2(x)\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels = 100, bias=True):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.conv1 = ResBlock(in_channels, 100, 11)\n",
    "        self.conv2 = ResBlock(100, 100, 11)\n",
    "        self.conv3 = ResBlock(100, 100, 11)\n",
    "        self.conv4 = ResBlock(100, 100, 11)\n",
    "#         self.conv5 = ResBlock(100, 100, 11)\n",
    "        self.conv5 = ConvBlock(nn.Conv1d, nn.PReLU, nn.BatchNorm1d,\n",
    "                        100, out_channels, 1,\n",
    "                        stride=1, padding=0, dilation=1)\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.conv2(h)\n",
    "        h = self.conv3(h)\n",
    "        h = self.conv4(h)\n",
    "        h = self.conv5(h)\n",
    "        return h\n",
    "    \n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels = 100, bias=True):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        \n",
    "#         self.att1 = SelfAttention(in_channels)\n",
    "        self.l_conv1 = ResBlock(in_channels, 100, 5)\n",
    "        self.l_conv2 = ResBlock(100, out_channels, 5)\n",
    "        self.l_pool1 = nn.MaxPool1d(2) # 64\n",
    "\n",
    "        self.m_conv1 = ResBlock2(out_channels, out_channels // 2, 5)\n",
    "        self.m_pool1 = nn.MaxPool1d(2) # 16\n",
    "        \n",
    "        self.g_conv1 = ResBlock2(out_channels, out_channels, 5)\n",
    "        self.g_pool1 = nn.MaxPool1d(2) # 16\n",
    "        self.g_conv2 = ResBlock2(out_channels, out_channels // 2, 5)\n",
    "        self.g_pool2 = nn.MaxPool1d(2) # 4\n",
    "\n",
    "        self.g_dense = nn.Linear(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, N = x.size()\n",
    "        h_l = self.l_conv1(x)\n",
    "        h_l = self.l_conv2(h_l)\n",
    "        h = self.l_pool1(h_l)\n",
    "        \n",
    "        h_m = self.m_pool1(self.m_conv1(h))\n",
    "        h_m = torch.cat([h_m[:,:,i:i+1].expand(-1, -1, N//h_m.size(-1)) for i in range(h_m.size(-1))], -1)\n",
    "\n",
    "        h_g = self.g_pool1(self.g_conv1(h))\n",
    "        h_g = self.g_pool2(self.g_conv2(h_g)).view(batch_size, -1)\n",
    "        h_g = self.g_dense(h_g).view(batch_size, -1, 1)\n",
    "        h_g = h_g.expand(-1, -1, N)\n",
    "\n",
    "        return F.softmax(torch.cat([h_l, h_m, h_g], 1)\n",
    "    \n",
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels = 100, N = 128, bias=True):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.conv1 = ResBlock(in_channels, 100, 11)\n",
    "        self.conv2 = ResBlock(100, 100, 11)\n",
    "        self.lstm1 = nn.LSTM(input_size = 100, \n",
    "                            hidden_size = 50, \n",
    "                            num_layers = 2,\n",
    "                            bias = True,\n",
    "                            bidirectional = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = 100, \n",
    "                    hidden_size = 50, \n",
    "                    num_layers = 2,\n",
    "                    bias = True,\n",
    "                    bidirectional = True)\n",
    "\n",
    "        self.linear = nn.Conv1d(100, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.conv2(h)\n",
    "        h = h.permute(2, 0, 1)\n",
    "        h = self.lstm1(h)[0]\n",
    "        h = self.lstm2(h)[0]\n",
    "        return F.softmax(self.linear(h.permute(1, 2, 0)),1)\n",
    "\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels = 100, N = 128, bias=True):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.conv1 = ResBlock(in_channels, 100, 11)\n",
    "        self.conv2 = ResBlock(100, 100, 11)\n",
    "        self.lstm1 = nn.LSTM(input_size = 100, \n",
    "                            hidden_size = 50, \n",
    "                            num_layers = 2,\n",
    "                            bias = True,\n",
    "                            bidirectional = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = 100, \n",
    "                    hidden_size = 50, \n",
    "                    num_layers = 2,\n",
    "                    bias = True,\n",
    "                    bidirectional = True)\n",
    "\n",
    "        self.linear = nn.Conv1d(100, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.conv2(h)\n",
    "        h = h.permute(2, 0, 1)\n",
    "        h = self.lstm1(h)[0]\n",
    "        h = self.lstm2(h)[0]\n",
    "        return F.softmax(self.linear(h.permute(1, 2, 0)),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMLayer(Layer):\n",
    "    r\"\"\"\n",
    "    Layer of One Hot neurons linked by a Markov Chain\n",
    "\n",
    "    Args:\n",
    "        T (Numpy Array): Transition Matrix for the Markov Chain\n",
    "        N (Integer): Number of neurons\n",
    "        q (Integer): Number of values the neuron can take\n",
    "        name (String): Name of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, T, N = 100, q = 21, h = 100, name = \"layer0\"):\n",
    "        super(HMMLayer, self).__init__(name)\n",
    "        self.full_name = f\"MC_{name}\"\n",
    "        self.N = N\n",
    "        self.q = q\n",
    "        self.h = h\n",
    "        self.T = T.float()\n",
    "        self.mu = nn.Parameter(torch.rand(self.q, self.h).float()/self.h, requires_grad=True)\n",
    "        self.sig = nn.Parameter(torch.rand(self.q, self.h, self.h).float()/self.h, requires_grad=True)\n",
    "        \n",
    "    def E(self, x):\n",
    "        tau = taus(x, self.T, self.mu, None)\n",
    "#         mu = torch.cat([((tau[:,s:s+1,:] * Dx).sum(-1)/tau.sum(-1)).mean(0).view(1, -1) for s in range(self.q)],0)\n",
    "#         sig = torch.cat([((tau[:,s:s+1,:] * Dx).sum(-1)/tau.sum(-1)).mean(0).view(1, -1) for s in range(self.q)],0)\n",
    "\n",
    "\n",
    "        return tau\n",
    "    \n",
    "    def max_likelihood(self, x):\n",
    "        return calcul_tau(x, self.T, self.mu, max_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_pdf(x, mu, sigma = None):\n",
    "    N = mu.size(0)\n",
    "    y = x - mu\n",
    "    Sy = y\n",
    "    if sigma is not None:\n",
    "        Sy = F.linear(x, sigma)\n",
    "    logZ = (N/2)*math.log(2 * math.pi)\n",
    "    return -0.5*(y * Sy).sum(1) - logZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_op = lambda x : torch.sum(x, 1)\n",
    "max_op = lambda x : torch.max(x, dim = 1)[0]\n",
    "\n",
    "def alphas(Dx, T, mu, sigma, operator = sum_op):\n",
    "    batch_size, _, N = Dx.size()\n",
    "    k = mu.size(0)\n",
    "    log_alphas = torch.zeros(batch_size, k, N)\n",
    "    log_norm = torch.zeros(batch_size, N)\n",
    "    \n",
    "    log_alphas[:,:,0] = torch.cat([log_pdf(Dx[:,:,0], mu[j]).view(-1,1) for j in range(k)],1)\n",
    "    log_norm[:,0] = torch.log((torch.exp(log_alphas[:,:,0])).sum(1))\n",
    "    for s in range(1,N):\n",
    "        for j in range(k):\n",
    "            log_alphas[:,j,s] = log_norm[:, s-1] + log_pdf(Dx[:,:,s], mu[j]) \n",
    "            log_alphas[:,j,s] += torch.log(operator(torch.cat([torch.exp((log_alphas[:,i,s-1] - log_norm[:,s-1] + torch.log(T[i,j])).view(-1,1)) for i in range(k)],1)))\n",
    "        log_norm[:,s] = log_norm[:,s-1] + torch.log(torch.exp(log_alphas[:,:,s]-log_norm[:,s-1].view(-1, 1)).sum(1)) \n",
    "    return(log_alphas, log_norm)\n",
    "\n",
    "def betas(Dx, T, mu, sigma, operator = sum_op):\n",
    "    batch_size, q, N = Dx.size()\n",
    "    k = mu.size(0)\n",
    "    log_betas = torch.zeros(batch_size, k, N)\n",
    "    log_norm = torch.zeros(batch_size, N)\n",
    "    \n",
    "    log_norm[:, -1] = torch.log(torch.exp(log_betas[:, :, -1]).sum(1))\n",
    "    for s in range(2,N+1):\n",
    "        for j in range(k):\n",
    "            log_betas[:,j,-s] = log_norm[:,-s+1] + torch.log(operator(torch.cat(\n",
    "                    [torch.exp(\n",
    "                        log_pdf(Dx[:,:,-s+1], mu[i]) + log_betas[:,i,-s+1] - log_norm[:,-s+1] + torch.log(T[j,i])).view(-1,1) for i in range(k)],1)))\n",
    "        log_norm[:,-s] = log_norm[:,-s+1] + torch.log(torch.exp(log_betas[:,:,-s] - log_norm[:,-s+1].view(-1, 1)).sum(1))\n",
    "\n",
    "    return(log_betas, log_norm)\n",
    "\n",
    "def taus(Dx, T, mu, sigma, operator = sum_op):\n",
    "    batch_size, _, N = Dx.size()        \n",
    "    log_alp, lnorma = alphas(Dx, T, mu, sigma, operator)\n",
    "    log_bet, lnormb = betas(Dx, T, mu, sigma, operator)\n",
    "    tau = log_alp - lnorma.view(batch_size, 1, N) + log_bet - lnormb.view(batch_size, 1, N)\n",
    "\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): ResBlock(\n",
       "    (conv_1): ConvBlock(\n",
       "      (conv): Conv1d(77, 100, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (activation): LeakyReLU(negative_slope=0.2)\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv1d(77, 100, kernel_size=(1,), stride=(1,))\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): ResBlock(\n",
       "    (conv_1): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (activation): LeakyReLU(negative_slope=0.2)\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): ResBlock(\n",
       "    (conv_1): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (activation): LeakyReLU(negative_slope=0.2)\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv4): ResBlock(\n",
       "    (conv_1): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (activation): LeakyReLU(negative_slope=0.2)\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
       "      (normalization): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (conv): Conv1d(100, 3, kernel_size=(1,), stride=(1,))\n",
       "    (activation): PReLU(num_parameters=1)\n",
       "    (normalization): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, qx, qs, h = 128, 77, 3, 8\n",
    "# del model\n",
    "device = torch.device('cpu')\n",
    "\n",
    "x = OneHotLayer(torch.zeros(qx*N), N = N, q = qx, name = \"x\")\n",
    "s = HMMLayer(t, N = N, q = qs, h = h, name = \"ss\")\n",
    "\n",
    "Dx = ConvNet(qx, qs)\n",
    "Ds = I\n",
    "# model = EnergyModel(x, s, Dx, Ds).float()\n",
    "model = Dx\n",
    "# model = nn.Transformer(d_model = 77, \n",
    "#                        nhead = 7, \n",
    "#                        dim_feedforward = 256)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvNet' object has no attribute 'generate_square_subsequent_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3131152fdf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAA_TENS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAA_MAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m77\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mMASK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvNet' object has no attribute 'generate_square_subsequent_mask'"
     ]
    }
   ],
   "source": [
    "###### TRANSFORMER #######\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "AA_TENS = torch.tensor(AA_MAT).view(1, 1, 20, 77)\n",
    "MASK = model.generate_square_subsequent_mask(128)\n",
    "\n",
    "def to_seq(x):\n",
    "    return ((x.permute(1, 2, 0).reshape(16, 128, 1, 77).expand(16, 128, 20, 77) - AA_TENS)**2).sum(-1).argmin(-1)\n",
    "\n",
    "def hinge_loss(model, x, y, m = 1):\n",
    "    e = -model(x)\n",
    "    e_bar = torch.min(e+y*1e9, 1, keepdim=True).values.view(e.size(0), 1, \n",
    "                                                                 e.size(-1))\n",
    "    loss = F.relu(m+(e-e_bar)*y)[:,:,10:-10]\n",
    "    return loss.sum()/(e.size(0))\n",
    "\n",
    "\n",
    "def aa_acc(x, recon_x):\n",
    "    r\"\"\"\n",
    "    Evaluate the ratio of amino acids retrieved in the reconstructed sequences\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): true sequence(s)\n",
    "        recon_x (torch.Tensor): reconstructed sequence(s)\n",
    "    \"\"\"\n",
    "    x = x[:,10:-10]\n",
    "    recon_x = recon_x[:,10:-10]\n",
    "    return (x==recon_x).int().float().mean().item()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    mean_loss, mean_reg, mean_acc = 0, 0, 0\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x = data[0].float().permute(1, 0, 2).to(device)\n",
    "        s = data[1].float().permute(1, 0, 2).to(device)\n",
    "        length = data[2].int().to(device)\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x, x, memory_mask = MASK)\n",
    "        loss = F.mse_loss(p, x)\n",
    "        acc = aa_acc(to_seq(x), to_seq(p))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del x; del s; del p\n",
    "        # Metrics\n",
    "        mean_loss = (mean_loss*batch_idx + loss.item())/ (batch_idx+1)\n",
    "        mean_acc = (mean_acc*batch_idx + acc)/ (batch_idx+1)\n",
    "        m, s = int(time.time()-start)//60, int(time.time()-start)%60\n",
    "        print(f'''Train Epoch: {epoch} [{int(100*batch_idx/len(train_loader))}%] || Time: {m} min {s} || Loss: {mean_loss:.3f} || Acc: {mean_acc:.3f}''', end=\"\\r\")\n",
    "    \n",
    "def val(epoch):\n",
    "    mean_loss, mean_reg, mean_acc = 0, 0, 0\n",
    "    model.eval()\n",
    "    cm = np.zeros((3,3))\n",
    "    for batch_idx, data in enumerate(val_loader):\n",
    "        x = data[0].float().permute(1, 0, 2).to(device)\n",
    "        s = data[1].float().permute(1, 0, 2).to(device)\n",
    "        p = model(x, x, memory_mask = MASK)\n",
    "\n",
    "        # Optimization\n",
    "        loss = F.mse_loss(p, x)\n",
    "        acc = aa_acc(to_seq(x), to_seq(p))\n",
    "\n",
    "        # Metrics\n",
    "        mean_loss = (mean_loss*batch_idx + loss.item())/ (batch_idx+1)\n",
    "        mean_acc = (mean_acc*batch_idx + acc)/ (batch_idx+1)\n",
    "        \n",
    "\n",
    "        m, s = int(time.time()-start)//60, int(time.time()-start)%60\n",
    "    \n",
    "    print(f'''Val: {epoch} [100%] || Time: {m} min {s} || Loss: {mean_loss:.3f} || Acc: {mean_acc:.3f}        ''')\n",
    "    cm = (np.array(cm.T, dtype=np.float)/np.sum(cm, 1)).T\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CONVO ######\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def hinge_loss(model, x, y, m = 1):\n",
    "    e = -model(x)\n",
    "    e_bar = torch.min(e+y*1e9, \n",
    "                      1, keepdim=True).values.view(e.size(0), 1, \n",
    "                                                    e.size(-1))\n",
    "    loss = F.relu(m+(e-e_bar)*y)[:,:,10:-10]\n",
    "    return loss.sum()/(e.size(0))\n",
    "\n",
    "\n",
    "def aa_acc(x, recon_x):\n",
    "    r\"\"\"\n",
    "    Evaluate the ratio of amino acids retrieved in the reconstructed sequences\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): true sequence(s)\n",
    "        recon_x (torch.Tensor): reconstructed sequence(s)\n",
    "    \"\"\"\n",
    "    x = x[:, :, 10:-10]\n",
    "    recon_x = recon_x[:,:,10:-10]\n",
    "    empty = torch.max(x, 1)[0].view(-1)\n",
    "    x = torch.argmax(x, 1).view(-1)\n",
    "    recon_x = torch.argmax(recon_x, 1).view(-1)\n",
    "    return (((x==recon_x) * (empty!=0)).int().sum().item())/((empty!=0).int().sum().item())\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    mean_loss, mean_reg, mean_acc = 0, 0, 0\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x = data[0].float().permute(0, 2, 1).to(device)\n",
    "        s = data[1].float().permute(0, 2, 1).to(device)\n",
    "        length = data[2].int().to(device)\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss = hinge_loss(model, x, s)\n",
    "#         p = model(x)\n",
    "#         loss = F.cross_entropy(p, s.argmax(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(d_0[\"visible\"].argmax(-1)[0], d_f[\"visible\"].argmax(-1)[0])\n",
    "        acc = aa_acc(s, model(x))\n",
    "\n",
    "        del x; del s\n",
    "        # Metrics\n",
    "        mean_loss = (mean_loss*batch_idx + loss.item())/ (batch_idx+1)\n",
    "        mean_acc = (mean_acc*batch_idx + acc)/ (batch_idx+1)\n",
    "        m, s = int(time.time()-start)//60, int(time.time()-start)%60\n",
    "        print(f'''Train Epoch: {epoch} [{int(100*batch_idx/len(train_loader))}%] || Time: {m} min {s} || Loss: {mean_loss:.3f} || Acc: {mean_acc:.3f}''', end=\"\\r\")\n",
    "    \n",
    "def val(epoch):\n",
    "    mean_loss, mean_reg, mean_acc = 0, 0, 0\n",
    "    model.eval()\n",
    "    cm = np.zeros((3,3))\n",
    "    for batch_idx, data in enumerate(val_loader):\n",
    "        x = data[0].float().permute(0, 2, 1).to(device)\n",
    "        s = data[1].float().permute(0, 2, 1).to(device)\n",
    "        p = model(x)\n",
    "\n",
    "        # Optimization\n",
    "        loss = hinge_loss(model, x, s)\n",
    "#         loss = F.cross_entropy(p, s.argmax(1))\n",
    "        acc = aa_acc(s, p)\n",
    "        \n",
    "        cm += confusion_matrix(s.argmax(1).view(-1), \n",
    "                         p.argmax(1).view(-1), labels = [0,1,2])\n",
    "        # Metrics\n",
    "        mean_loss = (mean_loss*batch_idx + loss.item())/ (batch_idx+1)\n",
    "        mean_acc = (mean_acc*batch_idx + acc)/ (batch_idx+1)\n",
    "        \n",
    "\n",
    "        m, s = int(time.time()-start)//60, int(time.time()-start)%60\n",
    "    \n",
    "    print(f'''Val: {epoch} [100%] || Time: {m} min {s} || Loss: {mean_loss:.3f} || Acc: {mean_acc:.3f}           ''')\n",
    "    cm = (np.array(cm.T, dtype=np.float)/np.sum(cm, 1)).T\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0 [100%] || Time: 1 min 5 || Loss: 286.896 || Acc: 0.675           \n",
      "[[0.74933265 0.06660711 0.18406024]\n",
      " [0.18907586 0.48433181 0.32659233]\n",
      " [0.19557271 0.07252514 0.73190215]]\n",
      "Val: 1 [100%] || Time: 2 min 16 || Loss: 286.294 || Acc: 0.677           \n",
      "[[0.75359821 0.07031541 0.17608638]\n",
      " [0.22675492 0.54665797 0.22658712]\n",
      " [0.21026001 0.12369282 0.66604717]]\n",
      "Val: 2 [100%] || Time: 3 min 19 || Loss: 289.151 || Acc: 0.660           \n",
      "[[0.65579844 0.16627322 0.17792833]\n",
      " [0.09801595 0.73955127 0.16243278]\n",
      " [0.1161046  0.23279111 0.65110429]]\n",
      "Val: 3 [100%] || Time: 4 min 35 || Loss: 285.324 || Acc: 0.682           \n",
      "[[0.66965584 0.1220325  0.20831167]\n",
      " [0.11120562 0.66052145 0.22827293]\n",
      " [0.11656096 0.16416936 0.71926968]]\n",
      "Val: 4 [100%] || Time: 5 min 39 || Loss: 284.876 || Acc: 0.685           \n",
      "[[0.78808818 0.04182705 0.17008477]\n",
      " [0.20497161 0.45310399 0.3419244 ]\n",
      " [0.20370503 0.06088634 0.73540863]]\n",
      "Val: 5 [100%] || Time: 6 min 38 || Loss: 284.497 || Acc: 0.687           \n",
      "[[0.78767196 0.07069768 0.14163035]\n",
      " [0.17764858 0.5501292  0.27222222]\n",
      " [0.21741531 0.09858227 0.68400242]]\n",
      "Val: 6 [100%] || Time: 7 min 48 || Loss: 283.247 || Acc: 0.692           \n",
      "[[0.704961   0.10950284 0.18553616]\n",
      " [0.1111953  0.63383353 0.25497117]\n",
      " [0.13715559 0.13942854 0.72341587]]\n",
      "Val: 7 [100%] || Time: 9 min 1 || Loss: 284.510 || Acc: 0.674           \n",
      "[[0.63940237 0.130202   0.23039563]\n",
      " [0.10579209 0.61073339 0.28347451]\n",
      " [0.14334673 0.11848922 0.73816405]]\n",
      "Val: 8 [100%] || Time: 10 min 28 || Loss: 284.304 || Acc: 0.686           \n",
      "[[0.63112827 0.13973304 0.22913868]\n",
      " [0.06828124 0.64633437 0.2853844 ]\n",
      " [0.09848926 0.14947774 0.75203299]]\n",
      "Val: 9 [100%] || Time: 11 min 46 || Loss: 283.240 || Acc: 0.693           \n",
      "[[0.75017007 0.07554209 0.17428784]\n",
      " [0.16615623 0.59997786 0.23386591]\n",
      " [0.15921083 0.14262078 0.69816839]]\n",
      "Train Epoch: 10 [99%] || Time: 12 min 54 || Loss: 282.428 || Acc: 0.700\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d3270dc21877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2a05d2e0e42f>\u001b[0m in \u001b[0;36mval\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m#         loss = F.cross_entropy(p, s.argmax(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2a05d2e0e42f>\u001b[0m in \u001b[0;36mhinge_loss\u001b[0;34m(model, x, y, m)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     e_bar = torch.min(e+y*1e9, \n\u001b[1;32m      8\u001b[0m                       \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f7e8fffb7234>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f7e8fffb7234>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/These/torch-pgm/pgm/nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    train(i)\n",
    "    val(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model  | Train Acc  | Val Acc |\n",
    "|---|---|---|\n",
    "| HMM  | 0.57  | 0.54  |\n",
    "| HMM + 5-Conv  | 0.971  | 0.70 |\n",
    "|   |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(val_loader):\n",
    "    x = data[0].float().permute(0, 2, 1).to(device)\n",
    "    s = data[1].float().permute(0, 2, 1).to(device)\n",
    "    p = model(x)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
